
>> load dataset <<

** raw datasets
DatasetDict({
    train: Dataset({
        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],
        num_rows: 14041
    })
    validation: Dataset({
        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],
        num_rows: 3250
    })
    test: Dataset({
        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],
        num_rows: 3453
    })
})

** raw/train/4
{'id': '4', 'tokens': ['Germany', "'s", 'representative', 'to', 'the', 'European', 'Union', "'s", 'veterinary', 'committee', 'Werner', 'Zwingmann', 'said', 'on', 'Wednesday', 'consumers', 'should', 'buy', 'sheepmeat', 'from', 'countries', 'other', 'than', 'Britain', 'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.'], 'pos_tags': [22, 27, 21, 35, 12, 22, 22, 27, 16, 21, 22, 22, 38, 15, 22, 24, 20, 37, 21, 15, 24, 16, 15, 22, 15, 12, 16, 21, 38, 17, 7], 'chunk_tags': [11, 11, 12, 13, 11, 12, 12, 11, 12, 12, 12, 12, 21, 13, 11, 12, 21, 22, 11, 13, 11, 1, 13, 11, 17, 11, 12, 12, 21, 1, 0], 'ner_tags': [5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0]}

** raw/train/features/ner_tags
Sequence(feature=ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], names_file=None, id=None), length=-1, id=None)


>> tokenize dataset <<

** # ex: 31

** ex
[('Germany', 5), ("'s", 0), ('representative', 0), ('to', 0), ('the', 0), ('European', 3), ('Union', 4), ("'s", 0), ('veterinary', 0), ('committee', 0), ('Werner', 1), ('Zwingmann', 2), ('said', 0), ('on', 0), ('Wednesday', 0), ('consumers', 0), ('should', 0), ('buy', 0), ('sheepmeat', 0), ('from', 0), ('countries', 0), ('other', 0), ('than', 0), ('Britain', 5), ('until', 0), ('the', 0), ('scientific', 0), ('advice', 0), ('was', 0), ('clearer', 0), ('.', 0)]

** # ex/subwords: 39

** ex/subwords
['[CLS]', 'germany', "'", 's', 'representative', 'to', 'the', 'european', 'union', "'", 's', 'veterinary', 'committee', 'werner', 'z', '##wing', '##mann', 'said', 'on', 'wednesday', 'consumers', 'should', 'buy', 'sheep', '##me', '##at', 'from', 'countries', 'other', 'than', 'britain', 'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.', '[SEP]']


>> tokenize dataset / align labels <<

** labels
[5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0]

** word_ids
[None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 7, 8, 9, 10, 11, 11, 11, 12, 13, 14, 15, 16, 17, 18, 18, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, None]

** aligned_labels
[-100, 5, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, -100]

** ex/subwords/aligned
[('[CLS]', -100), ('germany', 5), ("'", 0), ('s', 0), ('representative', 0), ('to', 0), ('the', 0), ('european', 3), ('union', 4), ("'", 0), ('s', 0), ('veterinary', 0), ('committee', 0), ('werner', 1), ('z', 2), ('##wing', 2), ('##mann', 2), ('said', 0), ('on', 0), ('wednesday', 0), ('consumers', 0), ('should', 0), ('buy', 0), ('sheep', 0), ('##me', 0), ('##at', 0), ('from', 0), ('countries', 0), ('other', 0), ('than', 0), ('britain', 5), ('until', 0), ('the', 0), ('scientific', 0), ('advice', 0), ('was', 0), ('clearer', 0), ('.', 0), ('[SEP]', -100)]

** tokenized
{'input_ids': [[101, 7327, 19164, 2446, 2655, 2000, 17757, 2329, 12559, 1012, 102], [101, 2848, 13934, 102], [101, 9371, 2727, 1011, 5511, 1011, 2570, 102], [101, 1996, 2647, 3222, 2056, 2006, 9432, 2009, 18335, 2007, 2446, 6040, 2000, 10390, 2000, 18454, 2078, 2329, 12559, 2127, 6529, 5646, 3251, 5506, 11190, 4295, 2064, 2022, 11860, 2000, 8351, 1012, 102], [101, 2762, 1005, 1055, 4387, 2000, 1996, 2647, 2586, 1005, 1055, 15651, 2837, 14121, 1062, 9328, 5804, 2056, 2006, 9317, 10390, 2323, 4965, 8351, 4168, 4017, 2013, 3032, 2060, 2084, 3725, 2127, 1996, 4045, 6040, 2001, 24509, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, -100], [-100, 1, 2, -100], [-100, 5, 0, 0, 0, 0, 0, -100], [-100, 0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 5, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, -100]]}

101 [CLS]
2762 germany
1005 '
1055 s
4387 representative
2000 to
1996 the
2647 european
2586 union
1005 '
1055 s
15651 veterinary
2837 committee
14121 werner
1062 z
9328 ##wing
5804 ##mann
2056 said
2006 on
9317 wednesday
** tokenized datasets
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'chunk_tags', 'id', 'input_ids', 'labels', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 14041
    })
    validation: Dataset({
        features: ['attention_mask', 'chunk_tags', 'id', 'input_ids', 'labels', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 3250
    })
    test: Dataset({
        features: ['attention_mask', 'chunk_tags', 'id', 'input_ids', 'labels', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 3453
    })
})


>> load model <<


>> data collator <<


>> metric <<

** fake labels
[['B-LOC', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]

** fake preds
[['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]

** example `('seqeval',)` metric calculation
{'LOC': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 2}, 'ORG': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1}, 'PER': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1}, 'overall_precision': 0.0, 'overall_recall': 0.0, 'overall_f1': 0.0, 'overall_accuracy': 0.8064516129032258}


>> finetune <<

{'eval_loss': 0.14508627355098724, 'eval_precision': 0.7899040558220671, 'eval_recall': 0.8104933437744715, 'eval_f1': 0.8000662580752028, 'eval_accuracy': 0.9604587986718985, 'eval_runtime': 3.1661, 'eval_samples_per_second': 1026.51, 'eval_steps_per_second': 8.212, 'epoch': 1.0}
{'eval_loss': 0.08507780730724335, 'eval_precision': 0.8763581051716645, 'eval_recall': 0.90233806913525, 'eval_f1': 0.8891583530838341, 'eval_accuracy': 0.9763610656584111, 'eval_runtime': 3.234, 'eval_samples_per_second': 1004.949, 'eval_steps_per_second': 8.04, 'epoch': 2.0}
{'eval_loss': 0.07181277871131897, 'eval_precision': 0.8973404837474007, 'eval_recall': 0.9172166909050229, 'eval_f1': 0.9071697278158886, 'eval_accuracy': 0.9790776367420211, 'eval_runtime': 3.2347, 'eval_samples_per_second': 1004.726, 'eval_steps_per_second': 8.038, 'epoch': 3.0}
{'eval_loss': 0.06957673281431198, 'eval_precision': 0.9010496391865296, 'eval_recall': 0.9219152030428459, 'eval_f1': 0.9113630080176942, 'eval_accuracy': 0.9795542281601982, 'eval_runtime': 3.2765, 'eval_samples_per_second': 991.911, 'eval_steps_per_second': 7.935, 'epoch': 4.0}
{'loss': 0.1678, 'learning_rate': 1.8181818181818183e-06, 'epoch': 4.55}
{'eval_loss': 0.06817884743213654, 'eval_precision': 0.9085010447597053, 'eval_recall': 0.9241525897751426, 'eval_f1': 0.916259982253771, 'eval_accuracy': 0.9806821611832176, 'eval_runtime': 3.2278, 'eval_samples_per_second': 1006.893, 'eval_steps_per_second': 8.055, 'epoch': 5.0}
{'train_runtime': 135.9739, 'train_samples_per_second': 516.312, 'train_steps_per_second': 4.045, 'train_loss': 0.15693028970198197, 'epoch': 5.0}

>> predict <<

** prediction results
{'LOC': {'precision': 0.926459719142646, 'recall': 0.9576012223071046, 'f1': 0.9417731029301277, 'number': 2618}, 'MISC': {'precision': 0.7781350482315113, 'recall': 0.7863525588952072, 'f1': 0.7822222222222222, 'number': 1231}, 'ORG': {'precision': 0.8751200768491835, 'recall': 0.8861867704280155, 'f1': 0.8806186563557274, 'number': 2056}, 'PER': {'precision': 0.9683110094740282, 'recall': 0.976928147659855, 'f1': 0.9726004922067268, 'number': 3034}, 'overall_precision': 0.9085010447597053, 'overall_recall': 0.9241525897751426, 'overall_f1': 0.916259982253771, 'overall_accuracy': 0.9806821611832176}


>> eval <<

** RESULT
{'eval_loss': 0.06817884743213654, 'eval_precision': 0.9085010447597053, 'eval_recall': 0.9241525897751426, 'eval_f1': 0.916259982253771, 'eval_accuracy': 0.9806821611832176, 'eval_runtime': 3.1707, 'eval_samples_per_second': 1025.0, 'eval_steps_per_second': 8.2, 'epoch': 5.0}

