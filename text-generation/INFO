
 XXX ezt az egészet -> ml-dl.INFO -ba! XXX

 * 2021.12.15.
   i lényeg: seq2seq szerű BERT-es generálós izét szeretnék 
     korpuszjavításra: ékezetesítésre, OCR-javításra...
     -- töredezettség-mentesítésre asszem kiváló a sima BERT!

   !   _lehetséges irányok:_ 
     +0] seqclass pipa (= training_seqclass.py)
         -> kitalálni a tokclass howto-t (alább!)
     [1] bert-sklearn-ból áttelepíteni dolgokat,
         és ennek révén kipróbálni huBERT-tel
         az ottani tokclass dolgokat! -- ugye jobb lesz? :)
     [2] generáló modellt (seq2seq, BART, EncoderDecoder)
         csinálni vhogy és ékezetesítésre kipróbálni!
         -- mire még? POS?

   ! BERTGeneration -- ez izgi lehet = 2 BERT-ből oldja meg!
     = és akkor elég a magyarításhoz a huBERT!
     @ https://huggingface.co/docs/transformers/model_doc/bertgeneration
     ? 
     ? bele lehet esetleg simán tenni a huBERT-et és kész? (?)  _ITT_T
     ? 
     x 
     - [./bertgeneration.py] XXX
     - [./encoderdecoder.py] XXX
     x 
     ! aszondja, hogy "You should probably TRAIN..."
       kitaláltam, hogyan kell (sentence class-ra) finetune-olni,
       (ld. "howto finetune")
       most abban bízom, hogy az segít itt! (!!!) XXX
     ! lényegében tuti: Seq2SepTrainer kell! (!!!) XXX XXX XXX
       @ https://medium.com/analytics-vidhya/fine-tune-a-roberta-encoder-decoder-model-trained-on-mlm-for-text-generation-23da5f3c1858
         -> van a végén link a notebook-ra! (!) XXX :)
       izé, nem is annyira:
       @ https://discuss.huggingface.co/t/trainer-vs-seq2seqtrainer/3145
     x 
     e esetleg idetartozhat az alábbi:
       dl1:~/tmp/simpletransformers/scriptsi/minimal_encoder_decoder.py

   ! a BART eleve ilyen, hogy tud generálni -- próbálgatom!
     @ https://huggingface.co/docs/transformers/model_doc/bart
     + [./bart_maskfilling.py] = mask filling :)
     ! 
     ! itt van csomó hasznos példa -- nézzem végig! (!) XXX :)  _ITT_T
     ! 
     ? hogy magyarítsuk? Gy modelljével? (?) XXX :)

   +  _howto finetune_ 
     @ https://huggingface.co/docs/transformers/training (!)
       -- plusz: https://huggingface.co/course/chapter3/3?fw=pt / predict
     -> jobbra-fent: open in colab ->
     + [./training_seqclass.py] --  _kiválóan működik!_  :)
     i ez sequence classification,
       hogyan lehet token classification-t csinálni? (?)
       -> ld. az "érdemes elolvasni" részt :)
     ! ezt lehetne próbálgatni a régi bert-sklearn dolgokkal,
       csak itt mostmár huBERT-tel is mennie kéne! (!) XXX :)

   i érdemes elolvasni
     @ https://huggingface.co/docs/transformers/notebooks (!)
     ! hú, itt  _mindenféle feladatra van külön notebook! (!) XXX :)
     ! ez az alábbi kettő nagyon jól néz ki, részletes,  _olvassam:
       @ https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/text_classification.ipynb
         = [./seqclass.py]
         = ez igaziból kb. a [./training_seqclass.py], kicsit másképp
           -- az eredeti notebookban részletes magyarázat van!
         +1] futtatni a [./seqclass.py] -t
           + push_to_hub -hoz kell hf access token -- kiszedtem
         +2] összevetni: [./training_seqclass.py] vs [./seqclass.py]
           + mindkettőből áttettem a másikba az értelmes új dolgokat
             most mindkét script kb. ua felépítésű,
             és mindkettő kiválóan fut! (!) XXX :)
         +3] átcsinálni a tokclass.py-t a seqclass.py szerint
           + ennek az align a lényege!
             (amit a bert-sklearn biztos auto megcsinál...)
         +4] kipróbálni a tokclass, ahogy van -> fenti [0]
         +5] [./training_seqclass.py] és [./seqclass.py] teljesen egységes
         +6] [./training_seqclass.py] és [./seqclass.py] összeolvasztani 
         [7] tokclass [cos]/[ref] terv:
           + formátuma legyen a seqclass-é <- ezen már ne változtassak! (!) XXX :)
           + tokclass: ne legyen 2x az IGNORED dolog kiszámolva!
            _ITT_T [1]
           - legyen a tokclass-ban is olyan szép bemutató a végén,
              mint a seqclass-ban (!) XXX :)
         [8] kipróbálni a tokclass-t a saját adataimon -> fenti [1]
         [9] 2-3 elemből álló adatok ellenőrizni,
             hogy helyes a kiértékelés! (!) XXX :)
       @ https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/token_classification.ipynb
         = [./tokclass.py]
         ! ha ez megvan, akkor megvan a token-class
     ! ez is érdekes lehet,  _olvassam:
       @ https://github.com/huggingface/transformers/tree/master/examples/pytorch
     * preprocessing
       @ https://huggingface.co/transformers/preprocessing.html
     * big table of models
       @ https://huggingface.co/transformers/index.html#bigtable

   i BART / paraphrase
     + [./bart_paraphrase.py]
       kb. ua script -- csak finetune-olva parafrázisolásra
 
   i nem vili, hogy mikor kell a .to(device) cucc ahhoz,
     hogy hajlandó legyen a GPU-n számolni XXX
     x kell: bart_maskfilling.py, bart_paraphrase.py, encoderdecoder.py
     + nem kell: training_seqclass.py
     ? talán a Trainer teszi GPU-ra automatice? (?)

